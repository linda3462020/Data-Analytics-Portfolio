---
title: "Lab-06: Machine Learning, Bias, and Variance"
Name: Chu-Chun Ku
output:
  html_document:
  theme: simplex
  fig_caption: true
---

# Getting started
In this exercise you will create a simulation in which you understand how the (simulated) world works. Using data that are generated by this world, you will use machine learning (ML) to try to reverse engineer how the world works. You will calculate the mean squared error (MSE) of different ML algorithms and decompose the MSE into noise, bias, and variance components.

Start by loading the `tidyverse` package.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
```

# Simulation Setup

We will build a world in which outcomes $Y$ are related to a predictor variable $X$ according to the equation $Y = f(X) + \epsilon,$ where $\epsilon$ is a mean-zero normally distributed random variable and

$$ f(X) = -16 + 24X - 9X^2 + X^3. $$
Remember that $f$ is the conditional expectation function, describing the expected value of $Y$ given $X$. The following code chunk defines and plots this function for values of $X$ ranging from 0 to 6.

```{r}
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
  geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
```

We need our world to generate data. In some cases, we'll want to collect data where the world picks values of $X$ along with corresponding values of $Y$. In other cases, we might want to collect data for specific values of $X$, and let the world generate only the corresponding $Y$ values.

The function in the following code chunk can perform either task. The function always returns a data frame that contains the following variables:

- `x` -- The value of the predictor variable $X$ for each observation in the dataset
- `f` -- The true expected value of $Y$ given $X$ (we can only calculate this in a simulation, where we are omnicient)
- `epsilon` -- The noise term $\epsilon$ that causes realized outcomes to deviate from their expected values
- `y` -- The realized outcome, defined by $y = f(x) + \epsilon$

The function has two modes:

1. If you set the `at` argument equal to `NULL` and provide a positive value for `n`, the function will create a sample of `n` observations, and for each observation will randomly select a characteristic `x` in the interval 0 to 6.
2. If you set the `at` argument to be a vector of `x` characteristics, the function will create a sample of `n = length(x)` observations with characteristics defined by `x = at`.

Finally, you get to determine how much noise there is in this world via the `sd` argument, which sets the standard deviation used when randomly drawing values for the error term `epsilon`. For example, if you set `sd = 0`, there will be no noise, and outcomes will be perfectly determined given `x`. By contrast, when `sd` is large, outcomes will be only partly detmined by `x`.

```{r}
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
  if (is.null(at)) at <- runif(n = n, min = 0, max = 6) 
  tibble(x = at,
         f = f(x),
         epsilon = rnorm(n = length(x), mean = 0, sd = sd),
         y = f(x) + epsilon)
}
```


# Problem 1: Draw data samples

This problem helps you gain familiarity with how to draw data samples in the simulation using the `draw` function.

First, collect a data sample of `n = 5` observations, where the characteristics `x` are selected randomly from the interval 0 to 6 (i.e., using mode 1 described above). Set the standard deviation of `epsilon` equal to zero. Print the contents of this data frame.

```{r}
# `draw` mode 1: draw a sample of 5 observations, using random values of x

mode1 <- draw(n = 5, sd = 0)
mode1

```

Next, collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5. Set the standard deviation of `epsilon` equal to 10. Print the contents of this data frame.

```{r}
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
x = seq(0, 6, by = 0.5)
mode2 <- draw(at = x, sd = 10)
mode2
```

# Problem 2: Visualizing noise

In this problem, use the `draw` function along with `ggplot2` to visualize how noise obscures $f$, the systematic relationship between $X$ and $Y$. 

### No noise
In the following code chunk, collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5. Set the standard deviation of `epsilon` equal to zero. Create a single ggplot that has the following two layers:

1. First layer. A line plot of how `f` (the true expected value of `y` given `x`) varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A scatter plot of how `y` (the realized outcome) varies with `x`. Set the point color to be black, and size to be 3.

```{r}
# Plot f and y with a data sample of size 13, no noise (sd = 0)
x = seq(0, 6, by = 0.5)
mode3 <- draw(at = x, sd = 0)
mode3

ggplot() +
  geom_line(aes(x = mode3$x, y = mode3$f), color = "orange", linewidth = 1.5) +
  geom_point(aes(x = mode3$x, y = mode3$y), color = "black", size = 13)
```


### More noise
Repeat the exercise from the "no noise" scenario, except now set the standard deviation of `epsilon` to be equal to 10. 

```{r}
# Plot f and y with a data sample of size 13, more noise (sd = 10)
x = seq(0, 6, by = 0.5)
mode4 <- draw(at = x, sd = 10)
mode4

ggplot() +
  geom_line(aes(x = mode4$x, y = mode4$f), color = "orange", linewidth = 1.5) +
  geom_point(aes(x = mode4$x, y = mode4$y), color = "black", size = 13)

```


### Discuss
In which world do you expect machine learning to perform better? How would your conclusions change, if at all, if you collected data samples where the characteristics `x` were selected randomly instead of at fixed intervals? Discuss briefly, connecting your discussion to the patterns revealed in your plots.

Ans: Machine learning usually performs better in the world without noise, because, in the true world, the noise always exists. As we can see in the graph above, the line and plots almost stay in the same track in the graph without noise.
As my observation, there is likely not difference between x selected randomly or at fixed intervals, when the epsilon is zero. They would be difference if the epsilon is difference, for example, there are difference in the table of mode2.


# Problem 3: The Machine

### Setup 

In this problem, we will use machine learning to construct an estimate of $f$, based only on values of `x` and `y` observed in data samples generated by this world. The machine learning algorithm will employ linear regression. Despite its name, linear regression can be used to estimate many types of nonlinear relationships, including polynomial relationships. 

We will use the following R functions to estimate $f$ using linear regression and to make predictions. You can look up the help documentation for each to learn more.

- `lm` -- Fits linear models
- `poly` -- Constructs polynomials of a specified degree
   - Example 1: `y ~ poly(x, degree = 1, raw = TRUE)` builds a simple linear model of the form $y = \beta_0 + \beta_1 x + \epsilon$
   - Example 2: `y ~ poly(x, degree = 3, raw = TRUE)` builds a cubic model of the form $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \epsilon$
- `predict.lm` -- Uses the output of `lm` to predict the outcome for specified values of `x`

Our machine learning algorithm has three different arguments:

1. `test_data` -- The dataset, produced by `draw`, for which we want to predict the outcomes.
2. `training_data` -- The dataset, produced by `draw`, that will we use to estimate $f$ via linear regression.
3. `deg` -- Specifies the degree of the polynomial in $x$ for the estimate of `f`.

The function `f_hat` implements this algorithm. Given the needed inputs, the function returns a data frame equal to `test_data`, but with an additional column `y_hat` of the predicted outcomes.

```{r}
# Define f_hat, the machine learning algorithm. 
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
  y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>% 
    predict.lm(newdata = test_data)
  mutate(test_data, y_hat = y_hat)
}
```

### ML under no noise 

Run the ML algorithm under a scenario where the world is deterministic (no noise). To do so, implement the algorithm with the following inputs:

- `test_data` -- Collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5 and the standard deviation of `epsilon` equal to zero.
- `training_data` -- Collect a data sample of `n = 10` observations, where the characteristics `x` are selected randomly and the standard deviation of `epsilon` equal to zero.
- `deg` -- Set `f_hat` to be a cubic function of `x` (`deg = 3`).

Using the result generated by `f_hat`, create a single ggplot that has the following three layers:

1. First layer. A line plot of how `f` varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A line plot of how `y_hat` varies with `x`. Set the point color to be blue, size to be 3, and transparency to be 1/2.
3. Third layer. A scatter plot of how `y` varies with `x`. Set the point color to be black, and size to be 3.

```{r}
# Plot f with a training data and test data predictions
sd <- 0

f_hat <- function(test_data, training_data, deg) {
  y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>% 
    predict.lm(newdata = test_data)
  mutate(test_data, y_hat = y_hat)
}

x <- seq(0, 6, by = 0.5)
test_data1 <- draw(at = x, sd = 0)
training_data1 <- draw(n = 10, sd = 0)
deg1 <- 3

p_data <- f_hat(test_data = test_data1, training_data = training_data1, deg = deg1)
p_data

ggplot() +
  geom_line(aes(x = p_data$x, y = p_data$f), color = "orange", linewidth = 1.5) +
  geom_line(aes(x = p_data$x, y = p_data$y_hat), color = "blue", size = 3, alpha = 0.5) +
  geom_point(aes(x = p_data$x, y = p_data$y), color = "black", size = 3)

```

**Discuss:** How well did the prediction algorithm perform? Is there any bias or variance in this case? Would that change if you had forced `f_hat` estimate a simple linear function of `x` (i.e., set `deg = 1`)?
Ans:
The prediction algorithm performs well. The lines and the points are completely the same. There is no bias or variance in the case. 
If I change deg to 1, the blue line will change from a curve to a straight line.

### ML under noise 

Repeat the exercise of "ML under no noise," except this time add noise to the world by setting the standard deviation of $\epsilon$ to be equal to 10 when drawing the `test_data` and `training_data`. How do the results and your discussion points change?

```{r}
# Plot f with a training data and test data predictions
sd <- 10

f_hat <- function(test_data, training_data, deg) {
  y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>% 
    predict.lm(newdata = test_data)
  mutate(test_data, y_hat = y_hat)
}

x <- seq(0, 6, by = 0.5)
test_data1 <- draw(at = x, sd = 10)
training_data1 <- draw(n = 10, sd = 10)
deg1 <- 3

p_data <- f_hat(test_data = test_data1, training_data = training_data1, deg = deg1)
p_data

ggplot() +
  geom_line(aes(x = p_data$x, y = p_data$f), color = "orange", linewidth = 1.5) +
  geom_line(aes(x = p_data$x, y = p_data$y_hat), color = "blue", size = 3, alpha = 0.5) +
  geom_point(aes(x = p_data$x, y = p_data$y), color = "black", size = 3)

```


# Problem 4: MSE

As a final step, you will calculate MSE from the machine learning algorithm. To do so, repeat the prediction part (not the visualization part) of the previous exercise "ML under noise" many times (i.e., `deg = 3`, `sd = 10`, etc.). Collect the prediction results as you go. Finally, using this collection of results, calculate how well the ML algorithm performed on average.

### Repeat the ML algorithm many times

In the following code chunk, use a `for` loop to repeat the prediction exercise 1,000 times. Save the output from the $i^\textit{th}$ iteration of the loop in the $i^\textit{th}$ position of the list `f_hat_list`. In the end, each element of this list will be the data frame returned by `f_hat`.

```{r}
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000

# Create an empty list to store the output from each iteration of the for loop
f_hat_list <- vector("list", iters)

for (i in 1:iters) {
  f_hat <- function(test_data, training_data, deg) {
  y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>% 
    predict.lm(newdata = test_data)
  mutate(test_data, y_hat = y_hat)
  }

  x <- seq(0, 6, by = 0.5)
  test_data1 <- draw(at = x, sd = 10)
  training_data1 <- draw(n = 10, sd = 10)
  deg1 <- 3

  p_data <- f_hat(test_data = test_data1, training_data = training_data1, deg = deg1)
  f_hat_list[[i]] <- p_data
}

```

### Assess the quality of predictions

Now that you have the results of estimating the ML algorithm many times, create a data frame called `f_hat_sum` that summarizes prediction results, separately for each value of `x` in `test_data`. (Hints: combine the list of data frames into one long data frame using `bind_rows`, then use `group_by` and `summarise` as appropriate to create the requested summaries.)

The summary data frame should contain one row for each value of `x` in `test_data` and should contain the following variables:

- `y_hat_mean` -- The mean predicted outcome
- `f_mean` -- The mean of the `f` (the true expected value, for use as a benchmark)
- `mse` -- The mean squared error of predictions
- `noise` -- The mean of `epsilon` squared
- `bias_squared` -- The bias in the predictions, squared
- `variance` -- The variance in the predictions

Add code to the following code chunk to build this summary data set, and then print the contents. 

```{r message=FALSE, warning=FALSE}
f_hat_sum <- bind_rows(f_hat_list)

f_hat_sum <- f_hat_sum %>%
  group_by(x) %>%
  summarise(
    y_hat_mean = mean(y_hat),
    f_mean = mean(f),
    mse = mean((y - y_hat)^2),
    noise = mean((epsilon)^2),
    bias_squared = (mean(y_hat) - mean(f))^2,
    variance = var(y_hat)
  )

print(f_hat_sum)


```
Using the results, briefly discuss the following:

- Is it true that MSE = Noise + Bias^2 + Variance? No, when x = 0.0, MES != Noise + Bias^2 + Variance.
- Is MSE the same at all values of `x`? No, they are all difference.
- What component (noise, bias, or variance) is driving most of the MSE? Does it depend on the value of `x`? In this case, variance drives most of the MAE, and it is related to value of x.

### Visualize

Visualize the summary of prediction results. Using the summary data frame you just constructed, create a ggplot with the following two layers:

1. First layer. A line plot of how `f_mean` (which is the same as `f`) varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A line plot of how `y_hat_mean` varies with `x`. Set the point color to be blue, size to be 3, and transparency to be 1/2.

```{r}

ggplot() +
  geom_line(aes(x = f_hat_sum$x, y = f_hat_sum$f_mean), color = "orange", linewidth = 1.5) +
  geom_line(aes(x = f_hat_sum$x, y = f_hat_sum$y_hat_mean), color = "blue", size = 3, alpha = 0.5)

```

**Discuss:** What does this plot indicate about bias in the ML algorithm? What about variance?
Ans: A small gap between these lines shows low bias, and low across x-values indicates low variance.

# Problem 5 [optional]: Bias-Variance Tradeoff

On average, over values of $X$ from 0 to 6, and under the "ML under noise" (`sd` = 10) scenario, do you get a larger or smaller MSE when the ML algorithm estimates $f$ as a cubic function of $X$ (`deg = 3`, as done in Problem 4) or as a simple linear function of $X$ (`deg = 1`)? Which version of the algorithm leads to higher bias, and which to higher variance?

Discuss these findings and evaluate the following claim: if the true `f` is nonlinear, then a simple linear models will generally perform worse than a nonlinear model.

```{r message=FALSE, warning=FALSE}
# Average MSE when estimating f as a cubic function of x


# Average MSE when estimating f as a simple linear function of x

```









